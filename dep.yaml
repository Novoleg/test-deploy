apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: my-app
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      affinity:
        ###
        # данное условие affinity позволяет разместить поды только на нодах с метками e2e-az1 или e2e-az2 или e2e-az3
        # тем самым поды нашего деплоймента будут находиться в разных зонах, что позволит сделать максимальную отказоустойчивость нашего приложения
        ###
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/e2e-az-name
                operator: In
                values:
                - e2e-az1
                - e2e-az2
                - e2e-az3
        ###
        # данное условие affinity означает, чтобы scheduler старался разместить поды на тех нодах, где нет подов с меткой: app: my-app
        ###
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 10
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - my-app
          topologyKey: topology.kubernetes.io/zone
      containers:
      - name: my-app
        image: my-app:1.0.0
        ###
        # 
        ###
        resources:
          ###
          # поскольку приложение после первых запросов начинает использовать 0.1 CPU, то я считаю, что это минимальное кол-во для работы приоложения
          ###
          requests:
            memory: "128Mi"
            cpu: "100m"
          ###
          # поскольку приложению на первые запросы нужно больше чем 0.1 CPU, то в лимитах указывается максмимально возможное потребления ресурсов CPU контейнером
          # решил взять с запасом, потому что из ТЗ не известно сколько приложение потребляет максимально.
          ###
          limits:
            ###
            # поскольку сказано, что память ровно потребляет 128 Мб, то для экономии ресурсов request и лимты по памяти нужно сделать одинаковыми
            ###
            memory: "128Mi"
            cpu: "1"
        ###
        # для примера представим, что у нас есть приложение, у которого есть endpoint для проверки readiness, liveness и startup probe
        ###
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          ###
          # за проверку запустилось ли приложение будет отвечать startupProbe. LivenessProbe будет проверять каждые 5 секунд, работает ли наше приложение и в случае зависание, рестартанет его
          ###
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          ###
          # логика такая же, как и для liveness probe
          ###
          periodSeconds: 5
        ###
        # startupProbe как раз предназначены для приложений, которые долго запускаются. Данная проба будет проверять каждую секунду, 11 раз запустилось ли приложение. ё
        # в данном случае у приложения будет 15 секунд на то, чтобы запуститься, если оно уложится в это время и startupProbe пройдет, то потом будет выполнятсья livenessProbe.
        # если приложение не успеет за 15 секунд запуститься, то данная проба удаляет под и в зависимости от restartpolicy под либо будет в состоянии crashLoopBackOff
        # иcходя из задания лучше использовать startupProbe поскольку наше прилоежение может запустится за 5-10 секунд
        ###
        startupProbe:
          httpGet:
            path: /healthz
            port: 8080
          failureThreshold: 15
          periodSeconds: 1
        ports:
          - containerPort: 8080
